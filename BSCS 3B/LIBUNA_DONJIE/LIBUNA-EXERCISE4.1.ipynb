{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b902cdb",
   "metadata": {},
   "source": [
    "DONJIE LIBUNA | \n",
    "BSCS 3B A | NLP |\n",
    "02/27/2026 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8528471f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n",
      "                                      doc class\n",
      "0                       Free money now!!!  SPAM\n",
      "1                    Hi mom, how are you?   HAM\n",
      "2              Lowest price for your meds  SPAM\n",
      "3             Are we still on for dinner?   HAM\n",
      "4                 Win a free iPhone today  SPAM\n",
      "5   Let's catch up tomorrow at the office   HAM\n",
      "6                Meeting at 3 PM tomorrow   HAM\n",
      "7              Get 50% off, limited time!  SPAM\n",
      "8              Team meeting in the office   HAM\n",
      "9                  Click here for prizes!  SPAM\n",
      "10               Can you send the report?   HAM\n",
      "\n",
      "Total documents: 11\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "# Dataset\n",
    "data = [\n",
    "    [\"Free money now!!!\", \"SPAM\"],\n",
    "    [\"Hi mom, how are you?\", \"HAM\"],\n",
    "    [\"Lowest price for your meds\", \"SPAM\"],\n",
    "    [\"Are we still on for dinner?\", \"HAM\"],\n",
    "    [\"Win a free iPhone today\", \"SPAM\"],\n",
    "    [\"Let's catch up tomorrow at the office\", \"HAM\"],\n",
    "    [\"Meeting at 3 PM tomorrow\", \"HAM\"],\n",
    "    [\"Get 50% off, limited time!\", \"SPAM\"],\n",
    "    [\"Team meeting in the office\", \"HAM\"],\n",
    "    [\"Click here for prizes!\", \"SPAM\"],\n",
    "    [\"Can you send the report?\", \"HAM\"]\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=['doc', 'class'])\n",
    "print(\"Dataset:\")\n",
    "print(df)\n",
    "print(f\"\\nTotal documents: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f407095f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample preprocessing:\n",
      "Original: 'Free money now!!!'\n",
      "Tokens: ['free', 'money', 'now']\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# PART 1: MANUAL NAÏVE BAYES IMPLEMENTATION\n",
    "# ============================================\n",
    "\n",
    "# 1. Text Preprocessing Function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Convert text to lowercase and split into tokens\"\"\"\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and split into words\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    return tokens\n",
    "\n",
    "print(\"Sample preprocessing:\")\n",
    "print(f\"Original: '{data[0][0]}'\")\n",
    "print(f\"Tokens: {preprocess_text(data[0][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "577a4f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 45\n",
      "\n",
      "Vocabulary: ['3', '50', 'a', 'are', 'at', 'can', 'catch', 'click', 'dinner', 'for', 'free', 'get', 'here', 'hi', 'how', 'in', 'iphone', 'let', 'limited', 'lowest', 'meds', 'meeting', 'mom', 'money', 'now', 'off', 'office', 'on', 'pm', 'price', 'prizes', 'report', 's', 'send', 'still', 'team', 'the', 'time', 'today', 'tomorrow', 'up', 'we', 'win', 'you', 'your']\n",
      "\n",
      "Word counts for HAM: {'hi': 1, 'mom': 1, 'how': 1, 'are': 2, 'you': 2, 'we': 1, 'still': 1, 'on': 1, 'for': 1, 'dinner': 1, 'let': 1, 's': 1, 'catch': 1, 'up': 1, 'tomorrow': 2, 'at': 2, 'the': 3, 'office': 2, 'meeting': 2, '3': 1, 'pm': 1, 'team': 1, 'in': 1, 'can': 1, 'send': 1, 'report': 1}\n",
      "\n",
      "Word counts for SPAM: {'free': 2, 'money': 1, 'now': 1, 'lowest': 1, 'price': 1, 'for': 2, 'your': 1, 'meds': 1, 'win': 1, 'a': 1, 'iphone': 1, 'today': 1, 'get': 1, '50': 1, 'off': 1, 'limited': 1, 'time': 1, 'click': 1, 'here': 1, 'prizes': 1}\n"
     ]
    }
   ],
   "source": [
    "# 2. Method to Generate Bag of Words (for word frequency)\n",
    "def generate_bag_of_words(documents, labels):\n",
    "    \"\"\"\n",
    "    Generate bag of words for each class\n",
    "    Returns vocabulary and word counts per class\n",
    "    \"\"\"\n",
    "    # Initialize counters for each class\n",
    "    word_counts = {'HAM': Counter(), 'SPAM': Counter()}\n",
    "    \n",
    "    # Count words for each class\n",
    "    for doc, label in zip(documents, labels):\n",
    "        tokens = preprocess_text(doc)\n",
    "        word_counts[label].update(tokens)\n",
    "    \n",
    "    # Get complete vocabulary (unique words across all documents)\n",
    "    vocabulary = set()\n",
    "    for counter in word_counts.values():\n",
    "        vocabulary.update(counter.keys())\n",
    "    \n",
    "    return vocabulary, word_counts\n",
    "\n",
    "# Generate bag of words\n",
    "documents = df['doc'].tolist()\n",
    "labels = df['class'].tolist()\n",
    "vocabulary, word_counts = generate_bag_of_words(documents, labels)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "print(f\"\\nVocabulary: {sorted(vocabulary)}\")\n",
    "print(f\"\\nWord counts for HAM: {dict(word_counts['HAM'])}\")\n",
    "print(f\"\\nWord counts for SPAM: {dict(word_counts['SPAM'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7d20229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior Probabilities:\n",
      "P(HAM) = 6/11 = 0.5455\n",
      "P(SPAM) = 5/11 = 0.4545\n"
     ]
    }
   ],
   "source": [
    "# 3. Method to Calculate Prior Probabilities\n",
    "def calculate_prior(labels):\n",
    "    \"\"\"\n",
    "    Calculate prior probabilities for each class\n",
    "    P(class) = count(class) / total_documents\n",
    "    \"\"\"\n",
    "    total_docs = len(labels)\n",
    "    class_counts = Counter(labels)\n",
    "    \n",
    "    priors = {}\n",
    "    for class_name, count in class_counts.items():\n",
    "        priors[class_name] = count / total_docs\n",
    "    \n",
    "    return priors, class_counts\n",
    "\n",
    "# Calculate priors\n",
    "priors, class_counts = calculate_prior(labels)\n",
    "\n",
    "print(\"Prior Probabilities:\")\n",
    "print(f\"P(HAM) = {class_counts['HAM']}/{len(labels)} = {priors['HAM']:.4f}\")\n",
    "print(f\"P(SPAM) = {class_counts['SPAM']}/{len(labels)} = {priors['SPAM']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d4d6050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Likelihoods (first 5 words from vocabulary):\n",
      "\n",
      "Word: '3'\n",
      "  P(3|HAM) = 0.025316\n",
      "  P(3|SPAM) = 0.014925\n",
      "\n",
      "Word: '50'\n",
      "  P(50|HAM) = 0.012658\n",
      "  P(50|SPAM) = 0.029851\n",
      "\n",
      "Word: 'a'\n",
      "  P(a|HAM) = 0.012658\n",
      "  P(a|SPAM) = 0.029851\n",
      "\n",
      "Word: 'are'\n",
      "  P(are|HAM) = 0.037975\n",
      "  P(are|SPAM) = 0.014925\n",
      "\n",
      "Word: 'at'\n",
      "  P(at|HAM) = 0.037975\n",
      "  P(at|SPAM) = 0.014925\n"
     ]
    }
   ],
   "source": [
    "# 4. Method to Calculate Likelihood of Tokens\n",
    "def calculate_likelihood(vocabulary, word_counts, alpha=1):\n",
    "    \"\"\"\n",
    "    Calculate P(word|class) for each word in vocabulary and each class\n",
    "    Using Laplace (add-one) smoothing\n",
    "    P(word|class) = (count(word, class) + alpha) / (total_words_in_class + alpha * |vocabulary|)\n",
    "    \"\"\"\n",
    "    likelihoods = {}\n",
    "    \n",
    "    for class_name, counter in word_counts.items():\n",
    "        likelihoods[class_name] = {}\n",
    "        total_words = sum(counter.values())\n",
    "        vocab_size = len(vocabulary)\n",
    "        \n",
    "        for word in vocabulary:\n",
    "            word_count = counter[word]\n",
    "            # Laplace smoothing\n",
    "            likelihood = (word_count + alpha) / (total_words + alpha * vocab_size)\n",
    "            likelihoods[class_name][word] = likelihood\n",
    "    \n",
    "    return likelihoods\n",
    "\n",
    "# Calculate likelihoods\n",
    "likelihoods = calculate_likelihood(vocabulary, word_counts)\n",
    "\n",
    "print(\"Sample Likelihoods (first 5 words from vocabulary):\")\n",
    "sample_words = sorted(vocabulary)[:5]\n",
    "for word in sample_words:\n",
    "    print(f\"\\nWord: '{word}'\")\n",
    "    print(f\"  P({word}|HAM) = {likelihoods['HAM'][word]:.6f}\")\n",
    "    print(f\"  P({word}|SPAM) = {likelihoods['SPAM'][word]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07be18e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Naïve Bayes classifier trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# 5. Complete Manual Naïve Bayes Classifier\n",
    "class ManualNaiveBayes:\n",
    "    def __init__(self, alpha=1):\n",
    "        self.alpha = alpha  # Smoothing parameter\n",
    "        self.vocabulary = None\n",
    "        self.word_counts = None\n",
    "        self.priors = None\n",
    "        self.likelihoods = None\n",
    "        \n",
    "    def fit(self, documents, labels):\n",
    "        \"\"\"Train the classifier\"\"\"\n",
    "        # Generate bag of words\n",
    "        self.vocabulary, self.word_counts = generate_bag_of_words(documents, labels)\n",
    "        \n",
    "        # Calculate priors\n",
    "        self.priors, _ = calculate_prior(labels)\n",
    "        \n",
    "        # Calculate likelihoods\n",
    "        self.likelihoods = calculate_likelihood(self.vocabulary, self.word_counts, self.alpha)\n",
    "        \n",
    "    def predict(self, document):\n",
    "        \"\"\"\n",
    "        Predict class for a document\n",
    "        Using log probabilities to avoid underflow\n",
    "        \"\"\"\n",
    "        tokens = preprocess_text(document)\n",
    "        \n",
    "        # Calculate log posterior for each class\n",
    "        log_posteriors = {}\n",
    "        \n",
    "        for class_name in self.priors.keys():\n",
    "            # Start with log prior\n",
    "            log_posterior = np.log(self.priors[class_name])\n",
    "            \n",
    "            # Add log likelihoods for each token\n",
    "            for token in tokens:\n",
    "                if token in self.vocabulary:\n",
    "                    log_posterior += np.log(self.likelihoods[class_name][token])\n",
    "                else:\n",
    "                    # Handle unknown words with smoothing\n",
    "                    vocab_size = len(self.vocabulary)\n",
    "                    total_words = sum(self.word_counts[class_name].values())\n",
    "                    unknown_prob = self.alpha / (total_words + self.alpha * vocab_size)\n",
    "                    log_posterior += np.log(unknown_prob)\n",
    "            \n",
    "            log_posteriors[class_name] = log_posterior\n",
    "        \n",
    "        # Return class with highest posterior probability\n",
    "        predicted_class = max(log_posteriors, key=log_posteriors.get)\n",
    "        \n",
    "        return predicted_class, log_posteriors\n",
    "\n",
    "# Train the manual classifier\n",
    "manual_nb = ManualNaiveBayes(alpha=1)\n",
    "manual_nb.fit(documents, labels)\n",
    "print(\"Manual Naïve Bayes classifier trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3712fe84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MANUAL NAÏVE BAYES CLASSIFICATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Test Sentence 1: 'Limited offer, click here!'\n",
      "Predicted Class: SPAM\n",
      "Log Posteriors:\n",
      "  SPAM: -15.5278\n",
      "  HAM: -18.0839\n",
      "Probabilities:\n",
      "  P(SPAM|document) = 0.9280 (92.80%)\n",
      "  P(HAM|document) = 0.0720 (7.20%)\n",
      "\n",
      "Test Sentence 2: 'Meeting at 2 PM with the manager.'\n",
      "Predicted Class: HAM\n",
      "Log Posteriors:\n",
      "  SPAM: -30.2213\n",
      "  HAM: -26.9156\n",
      "Probabilities:\n",
      "  P(SPAM|document) = 0.0354 (3.54%)\n",
      "  P(HAM|document) = 0.9646 (96.46%)\n"
     ]
    }
   ],
   "source": [
    "# 6. Test Manual Classifier on Test Sentences\n",
    "test_sentences = [\n",
    "    \"Limited offer, click here!\",\n",
    "    \"Meeting at 2 PM with the manager.\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MANUAL NAÏVE BAYES CLASSIFICATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, test_doc in enumerate(test_sentences, 1):\n",
    "    predicted_class, log_posteriors = manual_nb.predict(test_doc)\n",
    "    \n",
    "    print(f\"\\nTest Sentence {i}: '{test_doc}'\")\n",
    "    print(f\"Predicted Class: {predicted_class}\")\n",
    "    print(f\"Log Posteriors:\")\n",
    "    for class_name, log_prob in log_posteriors.items():\n",
    "        print(f\"  {class_name}: {log_prob:.4f}\")\n",
    "    \n",
    "    # Calculate actual probabilities (normalized)\n",
    "    max_log = max(log_posteriors.values())\n",
    "    posteriors = {k: np.exp(v - max_log) for k, v in log_posteriors.items()}\n",
    "    total = sum(posteriors.values())\n",
    "    posteriors = {k: v/total for k, v in posteriors.items()}\n",
    "    \n",
    "    print(f\"Probabilities:\")\n",
    "    for class_name, prob in posteriors.items():\n",
    "        print(f\"  P({class_name}|document) = {prob:.4f} ({prob*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "813cb994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn Multinomial Naïve Bayes classifier trained successfully!\n",
      "\n",
      "Vocabulary size: 45\n",
      "Classes: ['HAM' 'SPAM']\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# PART 2: SCIKIT-LEARN IMPLEMENTATION\n",
    "# ============================================\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline with CountVectorizer and MultinomialNB\n",
    "sklearn_nb = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(lowercase=True, token_pattern=r'\\b\\w+\\b')),\n",
    "    ('classifier', MultinomialNB(alpha=1.0))\n",
    "])\n",
    "\n",
    "# Train the classifier\n",
    "sklearn_nb.fit(documents, labels)\n",
    "\n",
    "print(\"Scikit-learn Multinomial Naïve Bayes classifier trained successfully!\")\n",
    "print(f\"\\nVocabulary size: {len(sklearn_nb.named_steps['vectorizer'].vocabulary_)}\")\n",
    "print(f\"Classes: {sklearn_nb.named_steps['classifier'].classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d17331ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SCIKIT-LEARN MULTINOMIAL NAÏVE BAYES CLASSIFICATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Test Sentence 1: 'Limited offer, click here!'\n",
      "Predicted Class: SPAM\n",
      "Probabilities:\n",
      "  P(HAM|document) = 0.0838 (8.38%)\n",
      "  P(SPAM|document) = 0.9162 (91.62%)\n",
      "\n",
      "Test Sentence 2: 'Meeting at 2 PM with the manager.'\n",
      "Predicted Class: HAM\n",
      "Probabilities:\n",
      "  P(HAM|document) = 0.9781 (97.81%)\n",
      "  P(SPAM|document) = 0.0219 (2.19%)\n"
     ]
    }
   ],
   "source": [
    "# Test Scikit-learn Classifier on Test Sentences\n",
    "print(\"=\" * 60)\n",
    "print(\"SCIKIT-LEARN MULTINOMIAL NAÏVE BAYES CLASSIFICATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, test_doc in enumerate(test_sentences, 1):\n",
    "    # Predict class\n",
    "    predicted_class = sklearn_nb.predict([test_doc])[0]\n",
    "    \n",
    "    # Get probability estimates\n",
    "    probabilities = sklearn_nb.predict_proba([test_doc])[0]\n",
    "    classes = sklearn_nb.named_steps['classifier'].classes_\n",
    "    \n",
    "    print(f\"\\nTest Sentence {i}: '{test_doc}'\")\n",
    "    print(f\"Predicted Class: {predicted_class}\")\n",
    "    print(f\"Probabilities:\")\n",
    "    for class_name, prob in zip(classes, probabilities):\n",
    "        print(f\"  P({class_name}|document) = {prob:.4f} ({prob*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae799126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPARISON OF RESULTS\n",
      "============================================================\n",
      "\n",
      "Test Sentence 1: 'Limited offer, click here!'\n",
      "  Manual Implementation: SPAM\n",
      "  Scikit-learn:          SPAM\n",
      "  Results Match: ✓\n",
      "\n",
      "Test Sentence 2: 'Meeting at 2 PM with the manager.'\n",
      "  Manual Implementation: HAM\n",
      "  Scikit-learn:          HAM\n",
      "  Results Match: ✓\n"
     ]
    }
   ],
   "source": [
    "# Comparison of Both Approaches\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARISON OF RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, test_doc in enumerate(test_sentences, 1):\n",
    "    manual_pred, _ = manual_nb.predict(test_doc)\n",
    "    sklearn_pred = sklearn_nb.predict([test_doc])[0]\n",
    "    \n",
    "    match = \"✓\" if manual_pred == sklearn_pred else \"✗\"\n",
    "    \n",
    "    print(f\"\\nTest Sentence {i}: '{test_doc}'\")\n",
    "    print(f\"  Manual Implementation: {manual_pred}\")\n",
    "    print(f\"  Scikit-learn:          {sklearn_pred}\")\n",
    "    print(f\"  Results Match: {match}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
